# Critical Analysis: Agent Reading Order & Documentation Architecture

> **Date:** 11 January 2026
> **Author:** Claude (Opus 4.5)
> **Purpose:** Deep critical analysis of agent onboarding documentation flow
> **Method:** Multi-round red team, SME panel critique, reconciliation synthesis

---

## Part A: Is the Agent Reading Order Clear?

### Current State Analysis

Based on the comprehensive audit of 170+ markdown files, the current reading order is specified in **three separate locations**:

#### Location 1: CLAUDE.md (Root)
```
1. Read docs/CORE/AI_HANDOVER.md
2. Check docs/CORE/index/ (CD/PD/RQ status)
3. Check docs/CORE/IMPLEMENTATION_ACTIONS.md
4. Read docs/CORE/RESEARCH_QUESTIONS.md for active research
```

#### Location 2: AI_AGENT_PROTOCOL.md (Session Entry Protocol)
```
STEP 1: Context Acquisition (Read in order)
â–¡ CLAUDE.md â€” Project overview, constraints, routing
â–¡ AI_HANDOVER.md â€” What did the last agent do?
â–¡ index/CD_INDEX.md + index/PD_INDEX.md â€” Quick decision status lookup
â–¡ index/RQ_INDEX.md â€” Quick research status lookup
â–¡ IMPLEMENTATION_ACTIONS.md â€” Task quick status + navigation hub
â–¡ IMPACT_ANALYSIS.md â€” Cascade tracking ONLY
â–¡ PRODUCT_DECISIONS.md â€” Full details for PENDING decisions only
â–¡ RESEARCH_QUESTIONS.md â€” Master Task Tracker + ACTIVE research
â–¡ GLOSSARY.md â€” What do terms mean in this codebase?
â–¡ AI_CONTEXT.md â€” What's the current architecture?
â–¡ ROADMAP.md â€” What are the current priorities?
```

#### Location 3: IMPLEMENTATION_ACTIONS.md (Agent Entry Point Routing)
```
SESSION START:
1. Read CLAUDE.md (project root) â† PRIMARY ENTRY POINT
   â†“
2. Read AI_HANDOVER.md â† Previous session context
   â†“
3. Read index/CD_INDEX.md + index/PD_INDEX.md + index/RQ_INDEX.md â† Quick status
   â†“
4. Read THIS DOCUMENT (IMPLEMENTATION_ACTIONS.md) â† Task overview
   â†“
5. Read RESEARCH_QUESTIONS.md â†’ Master Implementation Tracker â† Detailed tasks
```

### Critical Problem Identification

| Problem | Severity | Evidence |
|---------|----------|----------|
| **Three contradictory specifications** | HIGH | CLAUDE.md lists 4 files, Protocol lists 11, IMPL_ACTIONS lists 5 |
| **Inconsistent ordering** | MEDIUM | GLOSSARY.md appears 9th in Protocol, not mentioned in others |
| **No clear "stop" condition** | HIGH | When has an agent read "enough"? |
| **Context overload risk** | HIGH | 11 files Ã— ~500 lines avg = 5,500+ lines before starting |
| **Missing dependency awareness** | MEDIUM | Agent doesn't know WHY this order matters |
| **No differentiation by task type** | MEDIUM | Same reading for research vs implementation vs audit |

### Quantitative Analysis

**Current Reading Burden:**

| File | Lines | Tokens (est.) | Critical? |
|------|-------|---------------|-----------|
| CLAUDE.md | 61 | ~300 | âœ… YES |
| AI_HANDOVER.md | 1,734 | ~8,500 | âœ… YES (recent only) |
| CD_INDEX.md | 81 | ~400 | âœ… YES |
| PD_INDEX.md | 131 | ~650 | âœ… YES |
| RQ_INDEX.md | 146 | ~700 | âœ… YES |
| IMPLEMENTATION_ACTIONS.md | 666 | ~3,300 | âœ… YES |
| IMPACT_ANALYSIS.md | 754 | ~3,700 | ðŸŸ¡ CONDITIONAL |
| PRODUCT_DECISIONS.md | 2,651 | ~13,000 | ðŸŸ¡ CONDITIONAL |
| RESEARCH_QUESTIONS.md | 4,164 | ~20,000 | ðŸ”´ TOO LARGE |
| GLOSSARY.md | 2,431 | ~12,000 | ðŸŸ¡ REFERENCE |
| AI_CONTEXT.md | 549 | ~2,700 | ðŸŸ¡ CONDITIONAL |
| ROADMAP.md | 510 | ~2,500 | ðŸŸ¡ CONDITIONAL |

**Total if reading all 11:** ~67,750 tokens (before any actual work)

**Problem:** This exceeds many context windows and wastes significant capacity on orientation.

---

## Initial Assessment: Reading Order Clarity

### Verdict: ðŸ”´ NOT CLEAR

**Reasoning:**

1. **Multiple conflicting sources** â€” An agent arriving at the codebase will find THREE different reading orders in three different files
2. **No single authoritative source** â€” CLAUDE.md claims to be the entry point but delegates to Protocol, which has different content
3. **Overwhelming volume** â€” Reading everything prescribed would consume 67k+ tokens
4. **Missing task-based routing** â€” Same path for all work types is inefficient
5. **No verification mechanism** â€” No way to confirm an agent "completed" onboarding correctly

---

## Red Team Panel: Critique of Initial Assessment

### Red Team Member 1: Adversarial Skeptic
> "Your analysis assumes agents SHOULD read all files. But what if the redundancy is intentional? Perhaps CLAUDE.md is for quick orientation, Protocol is for thorough onboarding, and IMPL_ACTIONS is for returning agents. You've conflated three different use cases."

**Valid point.** Counter-analysis:
- If these are different use cases, this should be EXPLICITLY stated
- Currently, Protocol says "EVERY session MUST begin with this checklist" â€” no exception for quick vs thorough
- No documentation distinguishes "new agent" from "returning agent" flows

### Red Team Member 2: Efficiency Critic
> "You cite 67k tokens as excessive, but agents don't need to READ every line â€” they can skim. The real question is: what's the MINIMUM viable context for correct task execution?"

**Valid point.** Counter-analysis:
- "Skimming" is undefined behavior â€” different agents will skim differently
- Risk of missing critical constraints (CDs) by skimming is HIGH
- Better solution: define explicit "read fully" vs "skim for X" instructions

### Red Team Member 3: Complexity Defender
> "170+ markdown files exist because the project IS complex. Simplifying the reading order might hide necessary complexity and lead to errors. The current system, while verbose, is safe."

**Valid point.** Counter-analysis:
- Safety through verbosity creates its own failure mode: agents skip overwhelmed
- "Safe" only if agents actually follow it â€” evidence suggests they don't
- Complexity should be managed, not inflicted

### Red Team Member 4: Implementation Realist
> "You've identified the problem but haven't proven it causes actual failures. Where's the evidence that confused reading order led to bad outcomes?"

**Valid point.** Counter-analysis:
- Session 22 audit found 104/116 tasks blocked because documentation was ahead of implementation â€” this could have been caught earlier with better onboarding
- No agent had previously verified that Phase A schema existed before planning Phase H
- Counter-evidence: the governance system IS working (31/40 RQs complete, 18/18 CDs confirmed)

---

## Four Improved Recommendations (Post-Red Team Round 1)

Based on red team critique, here are four improved recommendations:

### Recommendation 1: Unified Single-Source Reading Order

**Proposal:** Consolidate all reading order specifications into ONE location (CLAUDE.md) and remove duplicates from other files.

**Specification:**
```markdown
## Agent Reading Order (AUTHORITATIVE â€” DO NOT DUPLICATE)

### Tier 1: ALWAYS (Every Session)
1. CLAUDE.md (this file) â€” 2 min
2. AI_HANDOVER.md (latest session only) â€” 3 min
3. index/CD_INDEX.md â€” 1 min (LOCKED decisions)
4. index/PD_INDEX.md â€” 1 min (PENDING decisions)
5. index/RQ_INDEX.md â€” 1 min (research status)

### Tier 2: TASK-SPECIFIC
- If IMPLEMENTATION: + IMPLEMENTATION_ACTIONS.md
- If RESEARCH: + RESEARCH_QUESTIONS.md (active section only)
- If DECISION: + PRODUCT_DECISIONS.md (pending section only)
- If AUDIT: + All index files + IMPACT_ANALYSIS.md

### Tier 3: REFERENCE (As Needed)
- GLOSSARY.md â€” When encountering unknown terms
- AI_CONTEXT.md â€” When needing architecture details
- ROADMAP.md â€” When needing priority context
```

**Rationale:** One source of truth eliminates conflicting instructions.

---

### Recommendation 2: Layered Reading with Explicit Stop Conditions

**Proposal:** Define when an agent has read "enough" based on task type.

**Specification:**
```markdown
## Reading Sufficiency Criteria

### For Quick Tasks (<30 min)
âœ… SUFFICIENT when you can answer:
- What did the last agent do? (AI_HANDOVER)
- What CDs constrain my work? (CD_INDEX)
- Is my task blocked? (RQ_INDEX, PD_INDEX)

### For Implementation Tasks
âœ… SUFFICIENT when you can answer above PLUS:
- What phase does my task belong to? (IMPLEMENTATION_ACTIONS)
- What tasks are blocked by my task? (IMPACT_ANALYSIS)

### For Research Tasks
âœ… SUFFICIENT when you can answer above PLUS:
- What's the full research question? (RESEARCH_QUESTIONS)
- What prior research informs this? (RQ_INDEX dependencies)

### For Governance Tasks
âœ… SUFFICIENT when you can answer above PLUS:
- What protocols apply? (AI_AGENT_PROTOCOL)
- What decisions are pending? (PRODUCT_DECISIONS)
```

**Rationale:** Stop conditions prevent over-reading and under-reading.

---

### Recommendation 3: Reading Order Verification Checklist

**Proposal:** Add a machine-verifiable checklist that agents complete before proceeding.

**Specification:**
```markdown
## Session Start Verification (Copy into response)

I have read and can confirm:
- [ ] Last session summary: [one-line summary]
- [ ] Critical blocker status: [BLOCKED/CLEAR]
- [ ] My task's phase: [A/B/C/D/E/F/G/H]
- [ ] Relevant CDs: [list or "none"]
- [ ] Task dependencies: [list or "none"]

If ANY checkbox is unclear â†’ READ MORE before proceeding
```

**Rationale:** Forces agents to demonstrate comprehension, not just file access.

---

### Recommendation 4: Progressive Disclosure Architecture

**Proposal:** Restructure documentation to reveal detail progressively rather than all at once.

**Specification:**
```
LEVEL 0: CLAUDE.md (60 lines)
â”œâ”€â”€ Contains: Project identity, critical constraints, next action
â”œâ”€â”€ Links to: Level 1 documents
â””â”€â”€ Stop here for: orientation only

LEVEL 1: Index Files + AI_HANDOVER (500 lines)
â”œâ”€â”€ Contains: Current state snapshot, what's blocked/ready
â”œâ”€â”€ Links to: Level 2 documents when needed
â””â”€â”€ Stop here for: most implementation tasks

LEVEL 2: IMPLEMENTATION_ACTIONS + PRODUCT_DEVELOPMENT_SHEET (1,000 lines)
â”œâ”€â”€ Contains: Task details, decision context, phase overview
â”œâ”€â”€ Links to: Level 3 documents for deep dives
â””â”€â”€ Stop here for: complex implementation tasks

LEVEL 3: Full Documents (10,000+ lines)
â”œâ”€â”€ Contains: Complete research, full decision history, all protocols
â””â”€â”€ Access for: research tasks, audits, governance changes
```

**Rationale:** Agents access complexity only when needed, reducing cognitive load.

---

## Part B: Red Team of Four Recommendations (Individual)

### Red Team: Recommendation 1 (Unified Single-Source)

| Critic | Critique | Validity |
|--------|----------|----------|
| **Maintenance Burden** | Single source creates single point of failure â€” if CLAUDE.md is wrong, everything fails | MEDIUM â€” But multiple sources also create drift |
| **Loss of Context** | Protocol's reading order includes WHY each file matters â€” removing it loses context | HIGH â€” Need to preserve rationale somewhere |
| **Breaking Change** | Removing reading order from Protocol requires updating all agents' expectations | LOW â€” Agents adapt to authoritative sources |
| **Completeness Risk** | CLAUDE.md's brevity might omit edge cases that Protocol captured | MEDIUM â€” Need to ensure nothing lost |

**Verdict:** MODIFY â€” Keep single source but include rationale for each file.

---

### Red Team: Recommendation 2 (Stop Conditions)

| Critic | Critique | Validity |
|--------|----------|----------|
| **Subjectivity** | "Can you answer X?" is subjective â€” agents might think they can when they can't | HIGH â€” Need objective verification |
| **Task Classification** | Requires agents to correctly classify their task type first | MEDIUM â€” But this is learnable |
| **False Confidence** | Agents might check boxes without deep understanding | HIGH â€” Verification != comprehension |
| **Missing Edge Cases** | Doesn't cover hybrid tasks (research + implementation) | LOW â€” Can specify "use higher tier" |

**Verdict:** MODIFY â€” Add objective verification criteria, not just self-assessment.

---

### Red Team: Recommendation 3 (Verification Checklist)

| Critic | Critique | Validity |
|--------|----------|----------|
| **Ceremony Overhead** | Adding checklist to every session creates friction | MEDIUM â€” But prevents costly errors |
| **Gaming Risk** | Agents might copy-paste without actually verifying | HIGH â€” No enforcement mechanism |
| **Incompleteness** | Five checkboxes can't capture all necessary context | MEDIUM â€” But captures most critical |
| **Redundancy** | If Tier 1 reading is correct, checklist is redundant | LOW â€” Checklist IS the verification |

**Verdict:** ACCEPT with modification â€” Make checklist required in AI_HANDOVER.md entries.

---

### Red Team: Recommendation 4 (Progressive Disclosure)

| Critic | Critique | Validity |
|--------|----------|----------|
| **Restructuring Cost** | Requires significant documentation reorganization | HIGH â€” But one-time cost |
| **Navigation Complexity** | More levels = more places to get lost | MEDIUM â€” But each level is smaller |
| **Dependency Confusion** | Agent at Level 1 might not know they need Level 2 | MEDIUM â€” Explicit routing helps |
| **Information Hiding** | Critical info at Level 3 might be missed | HIGH â€” Need "escalation triggers" |

**Verdict:** MODIFY â€” Add explicit "escalation triggers" that tell agents when to go deeper.

---

## Part C: SME Panel Critique (5 World-Class Leaders)

### SME 1: Information Architecture Expert (Nielsen Norman Group Methodology)

> **Critique:** Your progressive disclosure model is sound but lacks the critical "scent of information" concept. Users (agents) need signals about what lies deeper before they decide to dive. Your Level 0 â†’ Level 1 transition has no scent â€” CLAUDE.md doesn't preview what's in the index files.

**Recommendation:**
- Add "preview snippets" to each level that summarize what the next level contains
- Example: CLAUDE.md should say "Index files contain: 18 locked CDs, 32 PDs (15 resolved), 40 RQs (31 complete)"
- This lets agents make informed decisions about whether to proceed

**Impact:** HIGH â€” Addresses navigation complexity critique from Red Team

---

### SME 2: Cognitive Load Theory Expert (Sweller, van MerriÃ«nboer)

> **Critique:** Your analysis correctly identifies cognitive overload (67k tokens) but misses the distinction between intrinsic, extraneous, and germane load. The reading order should minimize EXTRANEOUS load (navigation, finding info) while preserving GERMANE load (understanding project complexity).

**Recommendation:**
- Separate "navigation documents" from "content documents"
- Index files are navigation (low cognitive load) â€” read first
- Full documents are content (high cognitive load) â€” read only when needed
- Current mixing of navigation and content in single files (e.g., IMPLEMENTATION_ACTIONS has both routing AND task details) increases extraneous load

**Impact:** MEDIUM â€” Supports restructuring but requires file reorganization

---

### SME 3: Knowledge Management Systems Expert (DIKW Hierarchy)

> **Critique:** Your documentation conflates Data, Information, Knowledge, and Wisdom. Index files are Data (raw facts). Full documents are Information (contextualized facts). Protocols are Knowledge (how to act). Handover is Wisdom (judgment from experience). Reading order should follow DIKW.

**Recommendation:**
- Reorder reading to follow DIKW:
  1. Wisdom first: AI_HANDOVER.md (what did someone with experience decide?)
  2. Knowledge second: Key protocols (how should I act?)
  3. Information third: Relevant documents (what context do I need?)
  4. Data last: Index files for specific lookups

- This inverts your current proposal but aligns with how experts actually work

**Impact:** HIGH â€” Fundamentally challenges the "index first" assumption

---

### SME 4: Distributed Systems Expert (Consistency Models)

> **Critique:** You've identified a consistency problem (three sources of truth) but proposed eventual consistency (consolidate into one). For documentation, you need strong consistency with defined ownership. Who OWNS the reading order? Without ownership, drift will recur.

**Recommendation:**
- Designate CLAUDE.md as the OWNER of reading order
- Add explicit statement: "Reading order is OWNED by CLAUDE.md. Other files may REFERENCE but not REDEFINE."
- Add versioning: "Reading Order v1.0 â€” Last updated: [date]"
- Add change control: "Changes to reading order require update to CLAUDE.md first"

**Impact:** HIGH â€” Addresses maintenance burden and drift prevention

---

### SME 5: Developer Experience (DevEx) Expert (Spotify/Google Engineering)

> **Critique:** Your recommendations focus on what agents SHOULD read but ignore the developer experience of MAINTAINING 170+ documents. Reading order clarity is a symptom; the disease is documentation sprawl. Every new analysis doc, reconciliation doc, and prompt adds to the burden.

**Recommendation:**
- Implement documentation budgets: Max 15 CORE documents, everything else is reference
- Implement documentation sunset: Analysis docs older than 30 days move to archive
- Implement documentation metrics: Track which docs are actually read (via agent logs)
- Consider: Is 170+ documents the RIGHT number? Or is this documentation debt?

**Impact:** CRITICAL â€” Addresses root cause, not just symptoms

---

## Part D: Reconciliation & Final Recommendation

### Synthesis of All Inputs

| Source | Key Insight | Weight |
|--------|-------------|--------|
| Initial Analysis | Three conflicting reading orders is the core problem | HIGH |
| Red Team Round 1 | Redundancy might serve different use cases | MEDIUM |
| Recommendation 1 | Single source of truth is necessary | HIGH |
| Recommendation 2 | Stop conditions prevent over/under-reading | HIGH |
| Recommendation 3 | Verification checklist ensures comprehension | MEDIUM |
| Recommendation 4 | Progressive disclosure manages complexity | HIGH |
| SME 1 (Info Arch) | Need "scent of information" between levels | HIGH |
| SME 2 (Cog Load) | Separate navigation from content | MEDIUM |
| SME 3 (DIKW) | Wisdom-first ordering (start with AI_HANDOVER) | HIGH |
| SME 4 (Distributed) | Need explicit ownership and versioning | HIGH |
| SME 5 (DevEx) | 170+ docs is the real problem (documentation debt) | CRITICAL |

### Counter-Narratives Considered

**Counter-Narrative 1:** "The current system works â€” 31/40 RQs complete, 18/18 CDs confirmed"

*Why I reject this:* Success metrics don't capture inefficiency. Tasks completed doesn't mean they were completed optimally. Session 22 audit revealed fundamental misalignment (Phase A missing) that better onboarding would have caught earlier.

**Counter-Narrative 2:** "Simplifying reading order will cause agents to miss critical context"

*Why I reject this:* The current system already causes agents to miss critical context through overwhelm. A structured, progressive system with explicit escalation triggers is MORE likely to surface critical context than an unstructured 11-file dump.

**Counter-Narrative 3:** "This is premature optimization â€” fix it when it breaks"

*Why I reject this:* It IS broken. The existence of three conflicting specifications IS the break. The fact that Session 22 had to discover missing schema through audit (rather than onboarding) IS the break.

### Alternative Proposals Considered

**Alternative A:** "Just pick one of the three existing orders and enforce it"

*Why I prefer my recommendation:* This doesn't address the underlying issues (cognitive load, stop conditions, verification). It's a band-aid.

**Alternative B:** "Create a fourth document specifically for reading order"

*Why I prefer my recommendation:* This adds to documentation sprawl (SME 5's concern) and creates yet another source of drift. Integration into CLAUDE.md is better.

**Alternative C:** "Automate reading order through tooling"

*Why I prefer my recommendation:* Good long-term solution but requires engineering investment. My recommendation is implementable TODAY with documentation changes only.

---

## FINAL RECOMMENDATION

### Reading Order Architecture v2.0

**Ownership:** CLAUDE.md is the SOLE AUTHORITATIVE SOURCE for reading order. All other files REFERENCE but do not REDEFINE.

**Structure:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT READING ORDER v2.0                                   â”‚
â”‚                    Owner: CLAUDE.md | Version: 2.0 | Date: 11 Jan 2026       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  LEVEL 0: ORIENTATION (Always â€” 5 min)                                â•‘  â”‚
â”‚  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â”‚
â”‚  â•‘  1. CLAUDE.md                    â†’ Project identity, constraints      â•‘  â”‚
â”‚  â•‘  2. AI_HANDOVER.md (top section) â†’ Last session wisdom               â•‘  â”‚
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  STOP if: Quick question, clarification only                         â•‘  â”‚
â”‚  â•‘  CONTINUE if: Any implementation, research, or audit task            â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                              â†“                                              â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  LEVEL 1: STATUS SNAPSHOT (Most tasks â€” 10 min)                       â•‘  â”‚
â”‚  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â”‚
â”‚  â•‘  3. index/CD_INDEX.md  â†’ 18 LOCKED decisions (constraints)           â•‘  â”‚
â”‚  â•‘  4. index/PD_INDEX.md  â†’ 32 decisions (15 resolved, 10 pending)      â•‘  â”‚
â”‚  â•‘  5. index/RQ_INDEX.md  â†’ 40 RQs (31 complete, 8+7 pending)           â•‘  â”‚
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  VERIFICATION: Can you state what CDs constrain your task?           â•‘  â”‚
â”‚  â•‘  STOP if: Task is unblocked and straightforward                      â•‘  â”‚
â”‚  â•‘  CONTINUE if: Task involves blocked items or dependencies            â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                              â†“                                              â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  LEVEL 2: TASK CONTEXT (Complex tasks â€” 15 min)                       â•‘  â”‚
â”‚  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â”‚
â”‚  â•‘  6. IMPLEMENTATION_ACTIONS.md  â†’ Task navigation, phase overview     â•‘  â”‚
â”‚  â•‘  7. PRODUCT_DEVELOPMENT_SHEET.md â†’ Executive summary, dependencies   â•‘  â”‚
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  VERIFICATION: Can you identify your task's phase and blockers?      â•‘  â”‚
â”‚  â•‘  STOP if: Implementation task with clear path                        â•‘  â”‚
â”‚  â•‘  CONTINUE if: Research task, governance task, or audit               â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                              â†“                                              â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  LEVEL 3: DEEP CONTEXT (Research/Audit only â€” 30+ min)                â•‘  â”‚
â”‚  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â”‚
â”‚  â•‘  8. RESEARCH_QUESTIONS.md (relevant section) â†’ Full task specs       â•‘  â”‚
â”‚  â•‘  9. PRODUCT_DECISIONS.md (pending section)   â†’ Decision rationale    â•‘  â”‚
â”‚  â•‘  10. AI_AGENT_PROTOCOL.md                    â†’ Behavioral rules      â•‘  â”‚
â”‚  â•‘  11. IMPACT_ANALYSIS.md                      â†’ Cascade effects       â•‘  â”‚
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  REFERENCE (as needed):                                              â•‘  â”‚
â”‚  â•‘  - GLOSSARY.md       â†’ Unknown terms                                 â•‘  â”‚
â”‚  â•‘  - AI_CONTEXT.md     â†’ Architecture questions                        â•‘  â”‚
â”‚  â•‘  - ROADMAP.md        â†’ Priority questions                            â•‘  â”‚
â”‚  â•‘  - docs/analysis/*   â†’ Prior reconciliation outputs                  â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SESSION START VERIFICATION (Required in first response)                    â”‚
â”‚                                                                             â”‚
â”‚  â–¡ Last session summary: ________________________________                   â”‚
â”‚  â–¡ Task classification: [Quick/Implementation/Research/Audit]               â”‚
â”‚  â–¡ Reading level reached: [0/1/2/3]                                        â”‚
â”‚  â–¡ Blocking CDs: __________________ (or "none")                            â”‚
â”‚  â–¡ Task dependencies: _____________ (or "none")                            â”‚
â”‚  â–¡ Phase: [A/B/C/D/E/F/G/H/N/A]                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Actions

| Action | File | Change |
|--------|------|--------|
| 1 | CLAUDE.md | Replace current reading order with v2.0 specification above |
| 2 | AI_AGENT_PROTOCOL.md | Remove Session Entry Protocol reading order; add reference: "See CLAUDE.md for authoritative reading order" |
| 3 | IMPLEMENTATION_ACTIONS.md | Remove Agent Entry Point Routing section; add reference: "See CLAUDE.md for authoritative reading order" |
| 4 | AI_HANDOVER.md | Add Session Start Verification template to "What Was Accomplished" section header |

### Why This Recommendation Over Alternatives

1. **Single Source** (from Recommendation 1) â€” Eliminates conflicting specifications
2. **Stop Conditions** (from Recommendation 2) â€” Prevents over/under-reading
3. **Verification** (from Recommendation 3) â€” Ensures comprehension
4. **Progressive Disclosure** (from Recommendation 4) â€” Manages cognitive load
5. **Information Scent** (from SME 1) â€” Each level previews what's in the next
6. **Wisdom-First** (from SME 3) â€” AI_HANDOVER comes early (Level 0)
7. **Explicit Ownership** (from SME 4) â€” CLAUDE.md owns, others reference
8. **Minimal Restructuring** (pragmatic) â€” Achievable with documentation changes only

### What This Doesn't Solve (Acknowledged Limitations)

- **Documentation Sprawl** (SME 5's concern) â€” 170+ docs still exist; this manages access, not volume
- **Enforcement** â€” No technical mechanism prevents agents from ignoring this
- **Evolution** â€” As project grows, levels may need adjustment
- **Tooling** â€” Long-term solution should automate this

---

## Self-Critique of Final Recommendation

### Weaknesses I Acknowledge

1. **Still Manual** â€” Relies on agent compliance, no automation
2. **Verification is Self-Reported** â€” Agents could lie on the checklist
3. **Level Boundaries are Subjective** â€” "Complex task" vs "straightforward task" requires judgment
4. **Doesn't Address Root Cause** â€” Documentation debt (170+ files) remains

### Improvements Absorbed from Self-Critique

1. Added explicit ownership statement (not just implicit)
2. Added version number for change tracking
3. Added "STOP if" and "CONTINUE if" criteria at each level
4. Added verification questions, not just checkboxes
5. Acknowledged limitations explicitly

---

## Red Team: Final Recommendation

### Final Red Team Challenge

> "You've created a beautiful specification that will be ignored like the current one. What's different this time?"

**Response:**

1. **Explicit ownership** â€” Current system has no owner; v2.0 designates CLAUDE.md
2. **Removal of duplicates** â€” Current system has three sources; v2.0 will have one (others reference)
3. **Verification requirement** â€” Current system has no checkpoint; v2.0 requires explicit verification
4. **Stop conditions** â€” Current system says "read everything"; v2.0 says "read enough"
5. **This document** â€” Creates audit trail of WHY this design was chosen, enabling future refinement

### Final Improvements Absorbed

- Added explicit statement that duplicates will be REMOVED, not just deprecated
- Added requirement that verification appears in agent's FIRST RESPONSE
- Added versioning (v2.0) to enable future evolution tracking

---

*Analysis complete: 11 January 2026*
*Total critique rounds: Initial + 4 Red Team + 5 SME = 10 perspectives integrated*
*Confidence: HIGH for reading order clarity improvement; MEDIUM for long-term adoption*
